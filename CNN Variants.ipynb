{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e1577a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "370ef94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('text_emotion.csv')\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs, mentions, hashtags, and special characters\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\@\\w+|\\#','', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to the content column\n",
    "df['content'] = df['content'].apply(preprocess_text)\n",
    "\n",
    "# Stopword removal and lemmatization\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def remove_stopwords_and_lemmatize(text):\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply stopword removal and lemmatization to the content column\n",
    "df['processed_content'] = df['content'].apply(remove_stopwords_and_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5315d601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (40000, 250)\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing and padding the sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Define maximum number of words to consider and maximum sequence length\n",
    "MAX_NB_WORDS = 10000\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "\n",
    "# Initialize and fit the tokenizer\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(df['processed_content'])\n",
    "\n",
    "# Convert the texts to sequences\n",
    "sequences = tokenizer.texts_to_sequences(df['processed_content'])\n",
    "\n",
    "# Pad the sequences\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print(\"Shape of data tensor:\", data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a550c490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 250, 100)          1000000   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 246, 128)          64128     \n",
      "                                                                 \n",
      " global_max_pooling1d (Glob  (None, 128)               0         \n",
      " alMaxPooling1D)                                                 \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1064257 (4.06 MB)\n",
      "Trainable params: 1064257 (4.06 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# CNN Model building\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "\n",
    "# Model parameters\n",
    "EMBEDDING_DIM = 100  # Dimension of the embedding layer\n",
    "\n",
    "# Building the CNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(1, activation='linear'))  # Assuming a single output for regression\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['mean_squared_error'])\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e42d4b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)        [(None, 250)]                0         []                            \n",
      "                                                                                                  \n",
      " embedding_4 (Embedding)     (None, 250, 100)             1000000   ['input_4[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_12 (Conv1D)          (None, 246, 128)             64128     ['embedding_4[0][0]']         \n",
      "                                                                                                  \n",
      " conv1d_13 (Conv1D)          (None, 246, 128)             82048     ['conv1d_12[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_14 (Conv1D)          (None, 246, 128)             82048     ['conv1d_13[0][0]']           \n",
      "                                                                                                  \n",
      " add_3 (Add)                 (None, 246, 128)             0         ['conv1d_12[0][0]',           \n",
      "                                                                     'conv1d_14[0][0]']           \n",
      "                                                                                                  \n",
      " global_max_pooling1d_4 (Gl  (None, 128)                  0         ['add_3[0][0]']               \n",
      " obalMaxPooling1D)                                                                                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 13)                   1677      ['global_max_pooling1d_4[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1229901 (4.69 MB)\n",
      "Trainable params: 1229901 (4.69 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Adding resudual to CNN\n",
    "from tensorflow.keras.layers import Input, add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Convert the emotion labels to numeric form using one-hot encoding\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(df['sentiment'])\n",
    "y = to_categorical(integer_encoded)\n",
    "\n",
    "# Update the number of output neurons to match the number of emotion categories\n",
    "num_emotions = y.shape[1]\n",
    "\n",
    "# Redefine the model with softmax output layer\n",
    "input_layer = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "embedding_layer = Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)(input_layer)\n",
    "conv_layer = Conv1D(128, 5, activation='relu')(embedding_layer)\n",
    "\n",
    "# Adding residual connection\n",
    "residual = Conv1D(128, 5, activation='relu', padding='same')(conv_layer)\n",
    "residual = Conv1D(128, 5, activation='relu', padding='same')(residual)\n",
    "residual = add([conv_layer, residual])\n",
    "\n",
    "pooling_layer = GlobalMaxPooling1D()(residual)\n",
    "output_layer = Dense(num_emotions, activation='softmax')(pooling_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a66a1320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "225/225 [==============================] - 49s 215ms/step - loss: 2.0461 - accuracy: 0.2923 - val_loss: 1.9492 - val_accuracy: 0.3328\n",
      "Epoch 2/10\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 1.8108 - accuracy: 0.3955 - val_loss: 1.9170 - val_accuracy: 0.3438\n",
      "Epoch 3/10\n",
      "225/225 [==============================] - 49s 218ms/step - loss: 1.5702 - accuracy: 0.4856 - val_loss: 2.0497 - val_accuracy: 0.3294\n",
      "Epoch 4/10\n",
      "225/225 [==============================] - 49s 218ms/step - loss: 1.2253 - accuracy: 0.6042 - val_loss: 2.3919 - val_accuracy: 0.3166\n",
      "Epoch 5/10\n",
      "225/225 [==============================] - 49s 217ms/step - loss: 0.8853 - accuracy: 0.7193 - val_loss: 2.8858 - val_accuracy: 0.2866\n",
      "Epoch 6/10\n",
      "225/225 [==============================] - 49s 218ms/step - loss: 0.6269 - accuracy: 0.8048 - val_loss: 3.5605 - val_accuracy: 0.2822\n",
      "Epoch 7/10\n",
      "225/225 [==============================] - 49s 218ms/step - loss: 0.4489 - accuracy: 0.8625 - val_loss: 4.3170 - val_accuracy: 0.2703\n",
      "Epoch 8/10\n",
      "225/225 [==============================] - 49s 220ms/step - loss: 0.3434 - accuracy: 0.8954 - val_loss: 5.0759 - val_accuracy: 0.2706\n",
      "Epoch 9/10\n",
      "225/225 [==============================] - 50s 223ms/step - loss: 0.2666 - accuracy: 0.9209 - val_loss: 5.7915 - val_accuracy: 0.2681\n",
      "Epoch 10/10\n",
      "225/225 [==============================] - 56s 248ms/step - loss: 0.2171 - accuracy: 0.9361 - val_loss: 6.4046 - val_accuracy: 0.2631\n",
      "250/250 [==============================] - 8s 33ms/step\n",
      "Accuracy: 0.26025\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 13 and the array at index 1 has size 104000",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\XUYINF~1\\AppData\\Local\\Temp/ipykernel_6508/3651371069.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mmse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mpearson_r\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorrcoef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# Print the evaluation metrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\core\\overrides.py\u001b[0m in \u001b[0;36mcorrcoef\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36mcorrcoef\u001b[1;34m(x, y, rowvar, bias, ddof, dtype)\u001b[0m\n\u001b[0;32m   2844\u001b[0m         warnings.warn('bias and ddof have no effect and are deprecated',\n\u001b[0;32m   2845\u001b[0m                       DeprecationWarning, stacklevel=3)\n\u001b[1;32m-> 2846\u001b[1;33m     \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcov\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrowvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2847\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2848\u001b[0m         \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\core\\overrides.py\u001b[0m in \u001b[0;36mcov\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36mcov\u001b[1;34m(m, y, rowvar, bias, ddof, fweights, aweights, dtype)\u001b[0m\n\u001b[0;32m   2638\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mrowvar\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2639\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2640\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2641\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2642\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mddof\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\core\\overrides.py\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 13 and the array at index 1 has size 104000"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, batch_size=128, epochs=10, validation_split=0.1, verbose=1)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49dcb036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.26025\n",
      "Mean Squared Error: 0.09468409419059753\n",
      "Pearson Correlation Coefficient: 0.17752963312942446\n",
      "Concordance Correlation Coefficient: 0.1772527530205531\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "accuracy = np.mean(np.argmax(y_test, axis=1) == np.argmax(y_pred, axis=1))\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "# Convert one-hot encoded y_test back to class labels\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Take the maximum predicted probability as the predicted class\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Compute Pearson correlation coefficient for the predicted class labels\n",
    "pearson_r = np.corrcoef(y_test_labels, y_pred_labels)[0, 1]\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Pearson Correlation Coefficient: {pearson_r}\")\n",
    "\n",
    "# Concordance Correlation Coefficient requires a custom implementation\n",
    "def concordance_correlation_coefficient(y_true, y_pred):\n",
    "    y_true_flat = np.argmax(y_true, axis=1)\n",
    "    y_pred_flat = np.argmax(y_pred, axis=1)\n",
    "    mean_true = np.mean(y_true_flat)\n",
    "    mean_pred = np.mean(y_pred_flat)\n",
    "    var_true = np.var(y_true_flat)\n",
    "    var_pred = np.var(y_pred_flat)\n",
    "    covariance = np.mean((y_true_flat - mean_true) * (y_pred_flat - mean_pred))\n",
    "    ccc = (2 * covariance) / (var_true + var_pred + (mean_true - mean_pred) ** 2)\n",
    "    return ccc\n",
    "\n",
    "ccc = concordance_correlation_coefficient(y_test, y_pred)\n",
    "print(f\"Concordance Correlation Coefficient: {ccc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735feb40",
   "metadata": {},
   "source": [
    "### Further tune hyperparameters and implement multi output CNN with convulution techniques: Gated convolutions and attention augmented convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b23c85e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
