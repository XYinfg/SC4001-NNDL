{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c2b3017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9f493e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8b2f7c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>processed_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>xoshayzers</td>\n",
       "      <td>i know  i was listenin to bad habit earlier a...</td>\n",
       "      <td>know listenin bad habit earlier started freaki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>wannamama</td>\n",
       "      <td>layin n bed with a headache  ughhhhwaitin on y...</td>\n",
       "      <td>layin n bed headache ughhhhwaitin call</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>coolfunky</td>\n",
       "      <td>funeral ceremonygloomy friday</td>\n",
       "      <td>funeral ceremonygloomy friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>czareaquino</td>\n",
       "      <td>wants to hang out with friends soon</td>\n",
       "      <td>want hang friend soon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>xkilljoyx</td>\n",
       "      <td>we want to trade with someone who has houston...</td>\n",
       "      <td>want trade someone houston ticket one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>1753918954</td>\n",
       "      <td>neutral</td>\n",
       "      <td>showMe_Heaven</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>1753919001</td>\n",
       "      <td>love</td>\n",
       "      <td>drapeaux</td>\n",
       "      <td>happy mothers day  all my love</td>\n",
       "      <td>happy mother day love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>1753919005</td>\n",
       "      <td>love</td>\n",
       "      <td>JenniRox</td>\n",
       "      <td>happy mothers day to all the mommies out there...</td>\n",
       "      <td>happy mother day mommy woman man long youre mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>1753919043</td>\n",
       "      <td>happiness</td>\n",
       "      <td>ipdaman1</td>\n",
       "      <td>wassup beautiful follow me  peep out my new h...</td>\n",
       "      <td>wassup beautiful follow peep new hit single de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>1753919049</td>\n",
       "      <td>love</td>\n",
       "      <td>Alpharalpha</td>\n",
       "      <td>bullet train from tokyo    the gf and i have ...</td>\n",
       "      <td>bullet train tokyo gf visiting japan since thu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id   sentiment         author  \\\n",
       "0      1956967341       empty     xoshayzers   \n",
       "1      1956967666     sadness      wannamama   \n",
       "2      1956967696     sadness      coolfunky   \n",
       "3      1956967789  enthusiasm    czareaquino   \n",
       "4      1956968416     neutral      xkilljoyx   \n",
       "...           ...         ...            ...   \n",
       "39995  1753918954     neutral  showMe_Heaven   \n",
       "39996  1753919001        love       drapeaux   \n",
       "39997  1753919005        love       JenniRox   \n",
       "39998  1753919043   happiness       ipdaman1   \n",
       "39999  1753919049        love    Alpharalpha   \n",
       "\n",
       "                                                 content  \\\n",
       "0       i know  i was listenin to bad habit earlier a...   \n",
       "1      layin n bed with a headache  ughhhhwaitin on y...   \n",
       "2                          funeral ceremonygloomy friday   \n",
       "3                    wants to hang out with friends soon   \n",
       "4       we want to trade with someone who has houston...   \n",
       "...                                                  ...   \n",
       "39995                                                      \n",
       "39996                     happy mothers day  all my love   \n",
       "39997  happy mothers day to all the mommies out there...   \n",
       "39998   wassup beautiful follow me  peep out my new h...   \n",
       "39999   bullet train from tokyo    the gf and i have ...   \n",
       "\n",
       "                                       processed_content  \n",
       "0      know listenin bad habit earlier started freaki...  \n",
       "1                 layin n bed headache ughhhhwaitin call  \n",
       "2                          funeral ceremonygloomy friday  \n",
       "3                                  want hang friend soon  \n",
       "4                  want trade someone houston ticket one  \n",
       "...                                                  ...  \n",
       "39995                                                     \n",
       "39996                              happy mother day love  \n",
       "39997  happy mother day mommy woman man long youre mo...  \n",
       "39998  wassup beautiful follow peep new hit single de...  \n",
       "39999  bullet train tokyo gf visiting japan since thu...  \n",
       "\n",
       "[40000 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('text_emotion.csv')\n",
    "\n",
    "# Preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs, mentions, hashtags, and special characters\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\@\\w+|\\#','', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to the content column\n",
    "df['content'] = df['content'].apply(preprocess_text)\n",
    "\n",
    "# Stopword removal and lemmatization\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def remove_stopwords_and_lemmatize(text):\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply stopword removal and lemmatization to the content column\n",
    "df['processed_content'] = df['content'].apply(remove_stopwords_and_lemmatize)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56ee538b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "# TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # You can adjust the number of features\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(df['content'])\n",
    "\n",
    "# Sentiment Intensity Scores using VADER\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "df['vader_score'] = df['content'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
    "\n",
    "# Combine TF-IDF features with VADER scores\n",
    "features = np.hstack((tfidf_features.toarray(), df['vader_score'].values.reshape(-1, 1)))\n",
    "\n",
    "# Now 'features' contains the TF-IDF vectors with VADER sentiment scores appended to each vector\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77cbe1d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>processed_content</th>\n",
       "      <th>vader_score</th>\n",
       "      <th>sentiment_intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>xoshayzers</td>\n",
       "      <td>i know  i was listenin to bad habit earlier a...</td>\n",
       "      <td>know listenin bad habit earlier started freaki...</td>\n",
       "      <td>-0.5423</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>wannamama</td>\n",
       "      <td>layin n bed with a headache  ughhhhwaitin on y...</td>\n",
       "      <td>layin n bed headache ughhhhwaitin call</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>coolfunky</td>\n",
       "      <td>funeral ceremonygloomy friday</td>\n",
       "      <td>funeral ceremonygloomy friday</td>\n",
       "      <td>-0.3612</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>czareaquino</td>\n",
       "      <td>wants to hang out with friends soon</td>\n",
       "      <td>want hang friend soon</td>\n",
       "      <td>0.4767</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>xkilljoyx</td>\n",
       "      <td>we want to trade with someone who has houston...</td>\n",
       "      <td>want trade someone houston ticket one</td>\n",
       "      <td>-0.3919</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>1753918954</td>\n",
       "      <td>neutral</td>\n",
       "      <td>showMe_Heaven</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>1753919001</td>\n",
       "      <td>love</td>\n",
       "      <td>drapeaux</td>\n",
       "      <td>happy mothers day  all my love</td>\n",
       "      <td>happy mother day love</td>\n",
       "      <td>0.8360</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>1753919005</td>\n",
       "      <td>love</td>\n",
       "      <td>JenniRox</td>\n",
       "      <td>happy mothers day to all the mommies out there...</td>\n",
       "      <td>happy mother day mommy woman man long youre mo...</td>\n",
       "      <td>0.5719</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>1753919043</td>\n",
       "      <td>happiness</td>\n",
       "      <td>ipdaman1</td>\n",
       "      <td>wassup beautiful follow me  peep out my new h...</td>\n",
       "      <td>wassup beautiful follow peep new hit single de...</td>\n",
       "      <td>0.5994</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>1753919049</td>\n",
       "      <td>love</td>\n",
       "      <td>Alpharalpha</td>\n",
       "      <td>bullet train from tokyo    the gf and i have ...</td>\n",
       "      <td>bullet train tokyo gf visiting japan since thu...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id   sentiment         author  \\\n",
       "0      1956967341       empty     xoshayzers   \n",
       "1      1956967666     sadness      wannamama   \n",
       "2      1956967696     sadness      coolfunky   \n",
       "3      1956967789  enthusiasm    czareaquino   \n",
       "4      1956968416     neutral      xkilljoyx   \n",
       "...           ...         ...            ...   \n",
       "39995  1753918954     neutral  showMe_Heaven   \n",
       "39996  1753919001        love       drapeaux   \n",
       "39997  1753919005        love       JenniRox   \n",
       "39998  1753919043   happiness       ipdaman1   \n",
       "39999  1753919049        love    Alpharalpha   \n",
       "\n",
       "                                                 content  \\\n",
       "0       i know  i was listenin to bad habit earlier a...   \n",
       "1      layin n bed with a headache  ughhhhwaitin on y...   \n",
       "2                          funeral ceremonygloomy friday   \n",
       "3                    wants to hang out with friends soon   \n",
       "4       we want to trade with someone who has houston...   \n",
       "...                                                  ...   \n",
       "39995                                                      \n",
       "39996                     happy mothers day  all my love   \n",
       "39997  happy mothers day to all the mommies out there...   \n",
       "39998   wassup beautiful follow me  peep out my new h...   \n",
       "39999   bullet train from tokyo    the gf and i have ...   \n",
       "\n",
       "                                       processed_content  vader_score  \\\n",
       "0      know listenin bad habit earlier started freaki...      -0.5423   \n",
       "1                 layin n bed headache ughhhhwaitin call       0.0000   \n",
       "2                          funeral ceremonygloomy friday      -0.3612   \n",
       "3                                  want hang friend soon       0.4767   \n",
       "4                  want trade someone houston ticket one      -0.3919   \n",
       "...                                                  ...          ...   \n",
       "39995                                                          0.0000   \n",
       "39996                              happy mother day love       0.8360   \n",
       "39997  happy mother day mommy woman man long youre mo...       0.5719   \n",
       "39998  wassup beautiful follow peep new hit single de...       0.5994   \n",
       "39999  bullet train tokyo gf visiting japan since thu...       0.0000   \n",
       "\n",
       "       sentiment_intensity  \n",
       "0                        0  \n",
       "1                        7  \n",
       "2                        7  \n",
       "3                       10  \n",
       "4                        2  \n",
       "...                    ...  \n",
       "39995                    2  \n",
       "39996                    9  \n",
       "39997                    9  \n",
       "39998                    8  \n",
       "39999                    9  \n",
       "\n",
       "[40000 rows x 7 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bcf16c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['empty' 'sadness' 'enthusiasm' 'neutral' 'worry' 'surprise' 'love' 'fun'\n",
      " 'hate' 'happiness' 'boredom' 'relief' 'anger']\n"
     ]
    }
   ],
   "source": [
    "unique_sentiments = df['sentiment'].unique()\n",
    "print(unique_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7c0ddb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "415141e9077d469faca0d94c49389a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Xu Yinfeng\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.texts[item])\n",
    "        label = self.labels[item]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# Load pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to convert categorical to numerical for regression\n",
    "emotion_intensity_map = {\n",
    "    'empty': 0,\n",
    "    'boredom': 1,\n",
    "    'neutral': 2,\n",
    "    'relief': 3,\n",
    "    'surprise': 4,\n",
    "    'fun': 5,\n",
    "    'worry': 6,\n",
    "    'sadness': 7,\n",
    "    'happiness': 8,\n",
    "    'love': 9,\n",
    "    'enthusiasm': 10,\n",
    "    'anger': 11,\n",
    "    'hate': 12\n",
    "}\n",
    "\n",
    "def convert_categorical_to_numerical(sentiment):\n",
    "    return emotion_intensity_map.get(sentiment, -1)\n",
    "\n",
    "df['sentiment_intensity'] = df['sentiment'].apply(convert_categorical_to_numerical)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(df['processed_content'], df['sentiment_intensity'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Define dataset parameters\n",
    "MAX_LEN = 128  \n",
    "BATCH_SIZE = 16  \n",
    "\n",
    "# Create datasets\n",
    "train_dataset = EmotionDataset(X_train.to_numpy(), y_train.to_numpy(), tokenizer, MAX_LEN)\n",
    "val_dataset = EmotionDataset(X_val.to_numpy(), y_val.to_numpy(), tokenizer, MAX_LEN)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Load pre-trained BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)  # num_labels=1 for regression\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fe9cc1e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Validation Loss: 7.1033\n",
      "Epoch 2/3, Validation Loss: 7.1712\n",
      "Epoch 3/3, Validation Loss: 7.4551\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "EPOCHS = 3  # Adjust as needed\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        loss = torch.nn.functional.mse_loss(outputs.logits.squeeze(-1), labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            loss = torch.nn.functional.mse_loss(outputs.logits.squeeze(-1), labels)\n",
    "            val_losses.append(loss.item())\n",
    "    \n",
    "    # Calculate validation loss\n",
    "    val_loss = np.mean(val_losses)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "model.save_pretrained('./emotion_intensity_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a6c822f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 7.4551\n",
      "Pearson Correlation Coefficient: 0.3318\n",
      "Concordance Correlation Coefficient (CCC): 0.2789\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Function to calculate Concordance Correlation Coefficient\n",
    "def concordance_correlation_coefficient(y_true, y_pred):\n",
    "    pearson_corr = pearsonr(y_true, y_pred)[0]\n",
    "    mean_true = np.mean(y_true)\n",
    "    mean_pred = np.mean(y_pred)\n",
    "    var_true = np.var(y_true)\n",
    "    var_pred = np.var(y_pred)\n",
    "    sd_true = np.sqrt(var_true)\n",
    "    sd_pred = np.sqrt(var_pred)\n",
    "    numerator = 2 * pearson_corr * sd_true * sd_pred\n",
    "    denominator = var_true + var_pred + (mean_true - mean_pred) ** 2\n",
    "    return numerator / denominator\n",
    "\n",
    "# Evaluation loop\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device).cpu().numpy()\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits.squeeze(-1).cpu().numpy()\n",
    "        \n",
    "        predictions.extend(logits)\n",
    "        true_labels.extend(labels)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "predictions = np.array(predictions)\n",
    "true_labels = np.array(true_labels)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mse = mean_squared_error(true_labels, predictions)\n",
    "pearson_corr, _ = pearsonr(true_labels, predictions)\n",
    "ccc = concordance_correlation_coefficient(true_labels, predictions)\n",
    "\n",
    "print(f'Mean Squared Error (MSE): {mse:.4f}')\n",
    "print(f'Pearson Correlation Coefficient: {pearson_corr:.4f}')\n",
    "print(f'Concordance Correlation Coefficient (CCC): {ccc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a780538b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c899f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52240355",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
